{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uebung 4 Mehrklassenklassifikation mit Neuronalen Netzen\n",
    "Beispielsweise Vorgehnsweise fuer zwei Klassen: Klasse '0' und Klasse '39'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globale Variablen\n",
    "Um hartcodierte Bezeichner/Namen in den Funktionen zu vermeiden, definiere an dieser Stelle alle Variablen, die global verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade zu bildern im jpg-Format\n",
    "\n",
    "# Pfad zu einem Ordner mit zwei Unterordnern: \"0\" und \"39\": die Ordnernamen entsprechen ClassIds\n",
    "# TO DO: Pfade entsprechend anpassen\n",
    "PATH_TO_TRAIN_DATA = 'path_to_folder\\im_0_39'\n",
    "PATH_TO_TEST_DATA = '.path_to_folder\\Images'\n",
    "PATH_TO_TEST_LABELS = '.path_to_folder\\GT-final_test.csv'\n",
    "\n",
    "# Pr√ºfe, ob der Pfad existiert / korrekt eingegeben wurde\n",
    "assert os.path.exists(PATH_TO_TRAIN_DATA), \"Der PATH_TO_TRAIN_DATA-Pfad existriert nicht.\"\n",
    "assert os.path.exists(PATH_TO_TEST_DATA), \"Der PATH_TO_TEST_DATA-Pfad existriert nicht.\"\n",
    "assert os.path.exists(PATH_TO_TEST_LABELS), \"Der PATH_TO_TEST_LABELS-Pfad existriert nicht.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = os.path.join(os.getcwd(), 'saved_models')\n",
    "MODEL_NAME = 'tr_sign_model_0_39.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = os.path.join(SAVE_DIR, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenaufbereitung\n",
    "### Training und Validation Daten\n",
    "Zur optimalen Nutzung des Arbeitsspeichers deines Rechners kannst du zum Laden von Daten die Funktion [image_dataset_from_directory](https://keras.io/api/preprocessing/image/#image_dataset_from_directory-function)  nutzen. Diese Funktion unterstuetzt jedoch nicht den ppm-Format. Daher ist fuer die Verwendung dieser Funktion die Ueberfuehrung der Daten in eins der unterstuetzten Formate ( jpeg, png, bmp, gif) notwendig.\n",
    "\n",
    "**HINWEIS**: In der TensorFlow-Version 2.1.0 und neuer ist die Funktion image_dataset_from_directory noch nicht vorhanden. Die TensorFlow-Version kann im Anaconda-Navigator gecheckt werden.\n",
    "\n",
    "Alternative [Moeglichkeiten](https://keras.io/api/preprocessing/image/): \n",
    "\n",
    "- ImageDataGenerator + flow_from_directory()\n",
    "- ImageDataGenerator + flow_from_dataframe()\n",
    "- load_img()\n",
    "- img_to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_size=(64, 64) fuer alle Bilder (optional) \n",
    "data_gen_train = ImageDataGenerator(rescale=1./255, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 408 images belonging to 2 classes.\n",
      "Found 102 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = data_gen_train.flow_from_directory(PATH_TO_TRAIN_DATA, target_size=(64, 64), \n",
    "                                                classes=['0', '39'], seed=1,\n",
    "                                                batch_size=64, subset='training')\n",
    "valid_data = data_gen_train.flow_from_directory(PATH_TO_TRAIN_DATA, target_size=(64, 64),\n",
    "                                                classes=['0', '39'], seed=1,\n",
    "                                                batch_size=64, subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Daten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_test = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test_data = pd.read_csv(PATH_TO_TEST_LABELS, sep=';', dtype=str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test_data = pd_test_data[['Filename', 'ClassId']]\n",
    "# nur zwei Klassen\n",
    "pd_test_data_0_39 = pd_test_data.loc[pd_test_data['ClassId'].isin(['0', '39'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>ClassId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>00243.ppm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>00252.ppm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>00277.ppm</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>00363.ppm</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>00403.ppm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Filename ClassId\n",
       "243  00243.ppm       0\n",
       "252  00252.ppm       0\n",
       "277  00277.ppm      39\n",
       "363  00363.ppm      39\n",
       "403  00403.ppm       0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_test_data_0_39.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = data_gen_test.flow_from_dataframe(pd_test_data_0_39, directory=PATH_TO_TEST_DATA, \n",
    "                                              x_col='Filename',y_col='ClassId',\n",
    "                                              classes=['0', '39'], seed=1,\n",
    "                                              target_size=(64, 64), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufbau des Modells\n",
    "Zum Aufbau deines Modells kannst du dich an die Modellarchitektur des [CIFAR-10 ConvNets](https://keras.io/examples/cifar10_cnn/) richten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), padding='same', input_shape=(64, 64, 3)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Conv2D(32, (3, 3)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Conv2D(64, (3, 3)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(NUM_CLASSES))\n",
    "model.add(layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kompilieren des Modells\n",
    "Eine detaillierte Beschreibung der [compile](https://keras.io/api/models/model_training_apis/#compile-method)-Methode findest du in Keras API Referenz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kompiliere das Modell\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training des Modells\n",
    "Eine detaillierte Beschreibung der [fit](https://keras.io/api/models/model_training_apis/#fit-method)-Methode findest du in Keras API Referenz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 7 steps, validate for 2 steps\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 60s 9s/step - loss: 0.6649 - accuracy: 0.7328 - val_loss: 0.3472 - val_accuracy: 0.9804\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 45s 6s/step - loss: 0.0843 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 38s 5s/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.2587e-07 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 37s 5s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.8136e-08 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 42s 6s/step - loss: 0.0335 - accuracy: 0.9902 - val_loss: 0.0024 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, epochs=5, validation_data=valid_data)\n",
    "# Die Warnung kannst du ignorieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6844388895175036,\n",
       "  0.07752976844123766,\n",
       "  0.002722420224659693,\n",
       "  0.002286339222622451,\n",
       "  0.03629595532513398],\n",
       " 'accuracy': [0.73284316, 1.0, 1.0, 1.0, 0.99019605],\n",
       " 'val_loss': [0.347248375415802,\n",
       "  0.002423337777145207,\n",
       "  2.2587000714224814e-07,\n",
       "  2.813573818372106e-08,\n",
       "  0.0024444700102321804],\n",
       " 'val_accuracy': [0.98039216, 1.0, 1.0, 1.0, 1.0]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des Modells\n",
    "Eine detaillierte Beschreibung der [evaluate](https://keras.io/api/models/model_training_apis/#evaluate-method)-Methode findest du in Keras API Referenz.\n",
    "\n",
    "Nach der Anwendung der *evaluate*-Methode kannst du dir zus√§tzlich den ausfuehrlichen Klassifikationsbericht (*classification_report()*) sowie die Konfusionsmatrix (*confusion_matrix()*) anschauen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "5/5 [==============================] - 4s 754ms/step - loss: 3.9670e-05 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data)\n",
    "# Die Warnung kannst du ignorieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.966986755585822e-05, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speichern des trainierten Modells\n",
    "Zum Speichern des trainierten Modells kann *save*-Methode\n",
    "Weiterfuehrende Informationen zu dieser Methode unter folgendem [Link](https://keras.io/api/models/model_saving_apis/) zu finden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichere das trainierte Modell, um dessen Wiederverwendung zu ermoeglichen\n",
    "model.save(FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nutzen des trainierten Modells zum Vorhersagen von Verkehrszeichen-Klassen \n",
    "Zum Wiederverwenden des trainierten gespeicherten Modells kann die [load_model](https://keras.io/api/models/model_saving_apis/#loadmodel-function)-Funktion verwendet werden.\n",
    "\n",
    "Eine detaillierte Beschreibung der [predict](https://keras.io/api/models/model_training_apis/#predict-method)-Methode findest du in Keras API Referenz. Diese Methodeist nur fuer Sequential-Modelle verfuegbar.\n",
    "\n",
    "An dieser Stelle werden die gleichen Bilder verwendet, die fuer die Evaluation des Modells verwendet wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade das trainierte Modell und teste die Erkennung der Verkehrszeichen-Klassen mit eigenen Beispielen\n",
    "saved_model = models.load_model(FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    '''\n",
    "    laedt Testbilder in Numpy Array\n",
    "    '''\n",
    "    img_data_list = []\n",
    "    pd_test_data = pd.read_csv(PATH_TO_TEST_LABELS, sep=';', dtype=str)\n",
    "    pd_test_data = pd_test_data[['Filename', 'ClassId']]\n",
    "    \n",
    "    # Zwei Klassen\n",
    "    pd_test_data_0_39 = pd_test_data.loc[pd_test_data['ClassId'].isin(['0', '39'])]\n",
    "    y_true = np.array(list(pd_test_data_0_39['ClassId']))\n",
    "    \n",
    "    # filenames_list\n",
    "    filenames_list = list(pd_test_data_0_39['Filename'])\n",
    "    for filename in filenames_list:\n",
    "        filepath = os.path.join(PATH_TO_TEST_DATA, filename)\n",
    "        assert os.path.exists(filepath), \"Der filepath existriert nicht.\"\n",
    "        \n",
    "        img = load_img(filepath, target_size=(64,64))\n",
    "        input_arr = img_to_array(img)\n",
    "        img_data_list.append(input_arr)\n",
    "    input_arr = np.array(img_data_list)\n",
    "    \n",
    "    # rescale-Operation\n",
    "    img_data = input_arr.astype(np.float32)*1./255\n",
    "    return y_true, img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClassIds im y_test-Array sind als Strings abgelegt: '0' und '39'\n",
    "y_test, x_test = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClassIds im y_test-Array sind als Integers abgelegt: 0 und 1\n",
    "predicted_classes = saved_model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping der ClassIds\n",
    "predicted_classes[predicted_classes == 1] = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        60\n",
      "          39       1.00      1.00      1.00        90\n",
      "\n",
      "    accuracy                           1.00       150\n",
      "   macro avg       1.00      1.00      1.00       150\n",
      "weighted avg       1.00      1.00      1.00       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluiere das trainierte Modell mit den Testdaten\n",
    "print(classification_report(y_true=y_test, y_pred=predicted_classes.astype(str), target_names=['0', '39']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=predicted_classes.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion_matrix(conf_matr, labels):\n",
    "    \n",
    "    plt.figure(figsize=(6,4))   \n",
    "    plt.imshow(conf_matr, interpolation='nearest')\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(NUM_CLASSES)\n",
    "    plt.xticks(tick_marks, ['0', '39'])\n",
    "    plt.yticks(tick_marks, tuple(labels))\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "\n",
    "plot_confusion_matrix(cm, ['0 - speed limit 20', '39 - keep left'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: \n",
    "# Modell fuer alle Klassen erweitern. Zur Optimierung der Ergebnisse andere Optimierer ausprobieren,\n",
    "# Modellarchitektur variieren ...\n",
    "# Optional: Google Colab einrichten und zum Training nutzen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
